{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Value iteration to find the optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3],\n",
       " [0, 1, 2, 3]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = [[0,1,2,3], [4,5,6,7], [8,9,10,11], [12,13,14,0]]   #0=Terminals\n",
    "A = [0, 1, 2, 3]      #0='up', 1='down', 2='right', 3='left'\n",
    "num_states = 14+2     #including terminals\n",
    "actions = []\n",
    "\n",
    "for i in range(0, num_states):\n",
    "    actions.append([0,1,2,3])\n",
    "\n",
    "actions     #possible moves/actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       " [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       " [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       " [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Considering probability 1 for a transition\n",
    "#For example, if action is DOWN and state is 1, there are probabilities of going DOWN or LEFT or RIGHT\n",
    "#So probability of going DOWN could be 0.8, LEFT could be 0.1 and RIGHT could be 0.1\n",
    "#But here we're considering that if action is DOWN, probability of the transition or going DOWN is 1 \n",
    "#because the model always picks up the highest probability\n",
    "\n",
    "flat_list = [item for sublist in actions for item in sublist] #flatten the list of lists\n",
    "num_actions = len(set(flat_list))\n",
    "trans_prob = [[[0 for i in range(num_states)] for j in range(num_states)] for k in range(num_actions)]\n",
    "\n",
    "#0=up\n",
    "trans_prob[0][0][0] = 1   #all actions in this terminal state transition to itself with probability 1\n",
    "trans_prob[0][1][1] = 1\n",
    "trans_prob[0][2][2] = 1\n",
    "trans_prob[0][3][3] = 1\n",
    "trans_prob[0][4][0] = 1   #0,4,0\n",
    "trans_prob[0][5][1] = 1\n",
    "trans_prob[0][6][2] = 1\n",
    "trans_prob[0][7][3] = 1\n",
    "trans_prob[0][8][4] = 1\n",
    "trans_prob[0][9][5] = 1\n",
    "trans_prob[0][10][6] = 1\n",
    "trans_prob[0][11][7] = 1\n",
    "trans_prob[0][12][8] = 1\n",
    "trans_prob[0][13][9] = 1\n",
    "trans_prob[0][14][10] = 1\n",
    "trans_prob[0][15][15] = 1 #all actions in this terminal state transition to itself with probability 1\n",
    "\n",
    "#1=down\n",
    "trans_prob[1][0][0] = 1   #all actions in this terminal state transition to itself with probability 1\n",
    "trans_prob[1][1][5] = 1\n",
    "trans_prob[1][2][6] = 1\n",
    "trans_prob[1][3][7] = 1\n",
    "trans_prob[1][4][8] = 1\n",
    "trans_prob[1][5][9] = 1\n",
    "trans_prob[1][6][10] = 1\n",
    "trans_prob[1][7][11] = 1\n",
    "trans_prob[1][8][12] = 1\n",
    "trans_prob[1][9][13] = 1\n",
    "trans_prob[1][10][14] = 1\n",
    "trans_prob[1][11][15] = 1  #1,11,0\n",
    "trans_prob[1][12][12] = 1\n",
    "trans_prob[1][13][13] = 1\n",
    "trans_prob[1][14][14] = 1\n",
    "trans_prob[1][15][15] = 1  #all actions in this terminal state transition to itself with probability 1\n",
    "\n",
    "#2=right\n",
    "trans_prob[2][0][0] = 1    #all actions in this terminal state transition to itself with probability 1\n",
    "trans_prob[2][1][2] = 1\n",
    "trans_prob[2][2][3] = 1\n",
    "trans_prob[2][3][3] = 1\n",
    "trans_prob[2][4][5] = 1\n",
    "trans_prob[2][5][6] = 1\n",
    "trans_prob[2][6][7] = 1\n",
    "trans_prob[2][7][7] = 1\n",
    "trans_prob[2][8][9] = 1\n",
    "trans_prob[2][9][10] = 1\n",
    "trans_prob[2][10][11] = 1\n",
    "trans_prob[2][11][11] = 1\n",
    "trans_prob[2][12][13] = 1\n",
    "trans_prob[2][13][14] = 1\n",
    "trans_prob[2][14][15] = 1  #2,14,0\n",
    "trans_prob[2][15][15] = 1  #all actions in this terminal state transition to itself with probability 1\n",
    "\n",
    "#3=left\n",
    "trans_prob[3][0][0] = 1    #all actions in this terminal state transition to itself with probability 1\n",
    "trans_prob[3][1][0] = 1    #3,1,0\n",
    "trans_prob[3][2][1] = 1\n",
    "trans_prob[3][3][2] = 1\n",
    "trans_prob[3][4][4] = 1\n",
    "trans_prob[3][5][4] = 1\n",
    "trans_prob[3][6][5] = 1\n",
    "trans_prob[3][7][6] = 1\n",
    "trans_prob[3][8][8] = 1\n",
    "trans_prob[3][9][8] = 1\n",
    "trans_prob[3][10][9] = 1\n",
    "trans_prob[3][11][10] = 1\n",
    "trans_prob[3][12][12] = 1\n",
    "trans_prob[3][13][12] = 1\n",
    "trans_prob[3][14][13] = 1\n",
    "trans_prob[3][15][15] = 1  #all actions in this terminal state transition to itself with probability 1\n",
    "\n",
    "trans_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-2, -2, -2, -2],\n",
       " [-2, -1, -1, -1],\n",
       " [-2, -1, -1, -1],\n",
       " [-2, -1, -2, -1],\n",
       " [-1, -1, -1, -2],\n",
       " [-1, -1, -1, -1],\n",
       " [-1, -1, -1, -1],\n",
       " [-1, -1, -2, -1],\n",
       " [-1, -1, -1, -2],\n",
       " [-1, -1, -1, -1],\n",
       " [-1, -1, -1, -1],\n",
       " [-1, -1, -2, -1],\n",
       " [-1, -2, -1, -2],\n",
       " [-1, -2, -1, -1],\n",
       " [-1, -2, -1, -1],\n",
       " [-2, -2, -2, -2]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initializing with lower reward value\n",
    "rewrds = [[-2 for i in range(num_actions)] for j in range(num_states)] \n",
    "\n",
    "#given that all valid transitions have -1 as reward\n",
    "for i in range(0, num_states):\n",
    "    for j in range(0, len(actions[i])):\n",
    "        rewrds[i][actions[i][j]] = -1\n",
    "\n",
    "##all actions in the terminal state transition to itself without any reward or lowest reward\n",
    "#set lower reward for terminals\n",
    "rewrds[0] = [-2,-2,-2,-2]    \n",
    "rewrds[15] = [-2,-2,-2,-2]\n",
    "\n",
    "#set lower reward for out of boundary actions\n",
    "#0=up\n",
    "rewrds[1][0] = -2\n",
    "rewrds[2][0] = -2\n",
    "rewrds[3][0] = -2\n",
    "rewrds[4][3] = -2\n",
    "\n",
    "#1=down\n",
    "rewrds[12][1] = -2\n",
    "rewrds[13][1] = -2\n",
    "rewrds[14][1] = -2\n",
    "\n",
    "#2=right\n",
    "rewrds[3][2] = -2\n",
    "rewrds[7][2] = -2\n",
    "rewrds[11][2] = -2\n",
    "\n",
    "\n",
    "#3=left\n",
    "rewrds[4][3] = -2\n",
    "rewrds[8][3] = -2\n",
    "rewrds[12][3] = -2\n",
    "\n",
    "rewrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def VI(giNumStates, gvActions, gvTrp, gvRews, gdDiscountFactor):\n",
    "    \n",
    "    t_dEpsilon = 0.000001\n",
    "    tmpv_vals = [0]*giNumStates\n",
    "    flat_list = [item for sublist in gvActions for item in sublist]\n",
    "    t_iNumActions = len(set(flat_list))\n",
    "    t_iCtr = 0\n",
    "    \n",
    "    while True:\n",
    "        currentv_vals = [0]*giNumStates\n",
    "        t_qValues = [[float(\"-inf\") for a in range(t_iNumActions)] for s in range(giNumStates)]\n",
    "        t_qValues[0] = [0, 0, 0, 0]\n",
    "        t_qValues[giNumStates - 1] = [0, 0, 0, 0]\n",
    "        \n",
    "        t_vPol = [float(\"-inf\") for s in range(giNumStates)]\n",
    "        for s in range(giNumStates):\n",
    "            if(s > 0 and s < giNumStates - 1):\n",
    "                for a in gvActions[s]:\n",
    "                    t_qValues[s][a] = gvRews[s][a]\n",
    "                    for ss in range(giNumStates):\n",
    "                        if(ss > 0 and ss < giNumStates - 1):\n",
    "                            t_qValues[s][a] += gdDiscountFactor*gvTrp[a][s][ss]*tmpv_vals[ss]\n",
    "        \n",
    "        currentv_vals = np.amax(np.around(t_qValues, 3), axis = 1)\n",
    "        t_vPol = np.argmax(np.around(t_qValues, 3), axis = 1)        \n",
    "        \n",
    "        \n",
    "        if np.linalg.norm(np.array(tmpv_vals) - np.array(currentv_vals)) < t_dEpsilon:\n",
    "            break\n",
    "                \n",
    "        tmpv_vals = currentv_vals  \n",
    "        t_iCtr = t_iCtr + 1\n",
    "        if(t_iCtr == 2): prev_pol = t_vPol\n",
    "                            \n",
    "    t_vPol[0] = -1\n",
    "    t_vPol[giNumStates - 1] = -1\n",
    "    prev_pol[0] = -1\n",
    "    prev_pol[giNumStates - 1] = -1\n",
    "    \n",
    "    return tmpv_vals, t_vPol, t_iCtr, prev_pol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "discount_factor = 1 #undiscounted\n",
    "\n",
    "start = time.time()\n",
    "state_values, opt_policy, iter_count, prev_pol = VI(num_states, actions, trans_prob, rewrds, discount_factor)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VALUE ITERATION\n",
      "\n",
      "Number of iterations required to achieve optimal policy:  3\n",
      "\n",
      "Time taken:  0.002\n",
      "\n",
      "State Values:\n",
      " 0  |  -1  |  -2  |  -3\n",
      "-----------------------\n",
      "-1  |  -2  |  -3  |  -2\n",
      "-----------------------\n",
      "-2  |  -3  |  -2  |  -1\n",
      "-----------------------\n",
      "-3  |  -2  |  -1  |  0\n",
      "-----------------------\n",
      "\n",
      "Optimal Policy: (-1 represents No Policy for Terminal States)\n",
      "Policy-0: UP, 1:DOWN, 2:RIGHT, 3:LEFT\n",
      "\n",
      "State:  0      Policy:  -1\n",
      "State:  1      Policy:  3\n",
      "State:  2      Policy:  3\n",
      "State:  3      Policy:  1\n",
      "State:  4      Policy:  0\n",
      "State:  5      Policy:  0\n",
      "State:  6      Policy:  0\n",
      "State:  7      Policy:  1\n",
      "State:  8      Policy:  0\n",
      "State:  9      Policy:  0\n",
      "State:  10      Policy:  1\n",
      "State:  11      Policy:  1\n",
      "State:  12      Policy:  0\n",
      "State:  13      Policy:  2\n",
      "State:  14      Policy:  2\n",
      "State:  15      Policy:  -1\n",
      "\n",
      "Actions\n",
      "END  |  <  |  <  |  v\n",
      "-----------------------\n",
      "  ^  |  ^  |  ^  |  v\n",
      "-----------------------\n",
      "  ^  |  ^  |  v  |  v\n",
      "-----------------------\n",
      "  ^  |  >  |  >  |  END\n",
      "-----------------------\n",
      "\n",
      "Policy from previous iteration (before last iteration):\n",
      "\n",
      "State:  0      Policy:  -1\n",
      "State:  1      Policy:  3\n",
      "State:  2      Policy:  1\n",
      "State:  3      Policy:  1\n",
      "State:  4      Policy:  0\n",
      "State:  5      Policy:  0\n",
      "State:  6      Policy:  0\n",
      "State:  7      Policy:  0\n",
      "State:  8      Policy:  0\n",
      "State:  9      Policy:  0\n",
      "State:  10      Policy:  0\n",
      "State:  11      Policy:  1\n",
      "State:  12      Policy:  0\n",
      "State:  13      Policy:  0\n",
      "State:  14      Policy:  2\n",
      "State:  15      Policy:  -1\n"
     ]
    }
   ],
   "source": [
    "print(\"VALUE ITERATION\")\n",
    "print()\n",
    "print(\"Number of iterations required to achieve optimal policy: \", iter_count)\n",
    "print()\n",
    "print(\"Time taken: \", round(end - start, 4))\n",
    "print()\n",
    "\n",
    "print(\"State Values:\")\n",
    "print(\"\", state_values[0], \" | \", state_values[1], \" | \", state_values[2], \" | \", state_values[3])\n",
    "print(\"-----------------------\")\n",
    "print(state_values[4], \" | \", state_values[5], \" | \", state_values[6], \" | \", state_values[7])\n",
    "print(\"-----------------------\")\n",
    "print(state_values[8], \" | \", state_values[9], \" | \", state_values[10], \" | \", state_values[11])\n",
    "print(\"-----------------------\")\n",
    "print(state_values[12], \" | \", state_values[13], \" | \", state_values[14], \" | \", state_values[15])\n",
    "print(\"-----------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Optimal Policy: (-1 represents No Policy for Terminal States)\")\n",
    "print(\"Policy-0: UP, 1:DOWN, 2:RIGHT, 3:LEFT\")\n",
    "print()\n",
    "for i in range(0, len(opt_policy)):\n",
    "    print(\"State: \", i, \"     Policy: \", opt_policy[i])\n",
    "\n",
    "print()\n",
    "\n",
    "actions_taken = [\"\" for x in range(len(opt_policy))]\n",
    "\n",
    "for i in range(0, len(opt_policy)):\n",
    "    if(opt_policy[i] == -1): actions_taken[i] = \"END\"\n",
    "    if(opt_policy[i] == 0): actions_taken[i] = \"^\"\n",
    "    if(opt_policy[i] == 1): actions_taken[i] = \"v\"\n",
    "    if(opt_policy[i] == 2): actions_taken[i] = \">\"\n",
    "    if(opt_policy[i] == 3): actions_taken[i] = \"<\"\n",
    "        \n",
    "print(\"Actions\")\n",
    "print(actions_taken[0], \" | \", actions_taken[1], \" | \", actions_taken[2], \" | \", actions_taken[3])\n",
    "print(\"-----------------------\")\n",
    "print(\" \", actions_taken[4], \" | \", actions_taken[5], \" | \", actions_taken[6], \" | \", actions_taken[7])\n",
    "print(\"-----------------------\")\n",
    "print(\" \", actions_taken[8], \" | \", actions_taken[9], \" | \", actions_taken[10], \" | \", actions_taken[11])\n",
    "print(\"-----------------------\")\n",
    "print(\" \", actions_taken[12], \" | \", actions_taken[13], \" | \", actions_taken[14], \" | \", actions_taken[15])\n",
    "print(\"-----------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Policy from previous iteration (before last iteration):\")\n",
    "print()\n",
    "for i in range(0, len(prev_pol)):\n",
    "    print(\"State: \", i, \"     Policy: \", prev_pol[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def QLearning(giNumStates, giNumActions, gvActions, gvTrp, gvRews, gdDiscountFactor, step_size=0.5):\n",
    "        \n",
    "    d_epsilon = 0.9 # randomly select an action with this prob\n",
    "    episodes = 500 # episodes for each run    \n",
    "    runs = 10 # perform 10 independent runs\n",
    "    print(\"Number of Iterations: \", runs)\n",
    "    print(\"Number of Episodes per Iteration: \", episodes)\n",
    "    print()\n",
    "\n",
    "    rewards_q_learning = np.zeros(episodes)\n",
    "    for r in tqdm(range(runs)):\n",
    "        print(\"begin-run\", r)\n",
    "        \n",
    "        q_value = np.zeros((giNumStates,giNumActions))\n",
    "        \n",
    "        for i in range(0, episodes):\n",
    "            state = 1 \n",
    "            rewards = 0\n",
    "            \n",
    "            while state > 0 and state < giNumStates - 1: #runs until terminal states have reached\n",
    "                if (np.random.random() < d_epsilon):\n",
    "                    action = np.random.choice(gvActions[state])\n",
    "                else:\n",
    "                    values_ = q_value[state, :]\n",
    "                    action = np.random.choice([action_ for action_, value_ in enumerate(values_) if value_ == np.max(values_)])\n",
    "\n",
    "                next_state = np.random.choice(np.arange(len(gvTrp[action][state])), p = gvTrp[action][state])\n",
    "                reward = gvRews[state][action]\n",
    "                \n",
    "                rewards += reward\n",
    "\n",
    "                #Q-Learning update\n",
    "                q_value[state, action] += step_size * (reward + gdDiscountFactor * np.max(q_value[next_state, :])-q_value[state, action])\n",
    "\n",
    "                state = next_state\n",
    "            \n",
    "            rewards_q_learning[i] += rewards\n",
    "    \n",
    "        if(r == runs-2): q_value_prev = q_value;\n",
    "        print(\"end-run\", r)\n",
    "        \n",
    "    rewards_q_learning /= runs\n",
    "    print(\"Rewards: \")\n",
    "    print(rewards_q_learning)\n",
    "    \n",
    "    optimal_policy = {}\n",
    "    prev_opt_pol = {}\n",
    "    for s in range(giNumStates):\n",
    "        optimal_policy[s] = np.argmax(q_value[s, :])\n",
    "        prev_opt_pol[s] = np.argmax(q_value_prev[s, :])\n",
    "                    \n",
    "    optimal_policy[0] = -1\n",
    "    optimal_policy[giNumStates - 1] = -1\n",
    "    prev_opt_pol[0] = -1\n",
    "    prev_opt_pol[giNumStates - 1] = -1\n",
    "    \n",
    "    print(\"Q-Values:\")\n",
    "    print(q_value)\n",
    "    \n",
    "    return q_value, optimal_policy, prev_opt_pol\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-LEARNING\n",
      "\n",
      "Number of Iterations:  10\n",
      "Number of Episodes per Iteration:  500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 0\n",
      "end-run 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [00:00<00:01,  7.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 1\n",
      "end-run 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [00:00<00:01,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 2\n",
      "end-run 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:00<00:00,  7.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 3\n",
      "end-run 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [00:00<00:00,  7.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 4\n",
      "end-run 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [00:00<00:00,  7.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 5\n",
      "end-run 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [00:00<00:00,  7.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 6\n",
      "end-run 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [00:00<00:00,  8.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 7\n",
      "end-run 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [00:01<00:00,  7.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 8\n",
      "end-run 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [00:01<00:00,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin-run 9\n",
      "end-run 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:01<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rewards: \n",
      "[ -9.8 -25.6 -21.  -20.6  -9.6 -17.  -10.8 -11.  -11.  -13.8  -5.6 -10.\n",
      " -17.8 -12.6 -13.6 -11.6 -11.4 -11.2  -7.  -12.2 -13.2  -7.4  -7.6 -17.4\n",
      " -11.   -7.6  -4.4 -22.2  -6.2  -9.4 -17.6 -11.6 -19.8  -4.6  -9.4 -17.8\n",
      " -12.6 -14.6  -8.  -11.   -9.   -3.4 -11.8  -6.6  -6.6 -13.2 -17.8 -10.8\n",
      " -16.4  -5.6 -16.   -4.2 -13.2  -5.2  -9.   -9.   -9.2 -18.2 -22.6  -7.8\n",
      "  -7.2 -12.6  -9.4  -6.2  -9.4 -25.8 -17.6 -15.4  -8.6  -4.6  -7.8  -2.2\n",
      "  -8.6 -22.8  -5.2  -9.8  -6.8 -15.  -10.  -13.6  -9.8  -5.  -12.2 -11.4\n",
      " -11.   -5.6 -16.4  -8.8 -14.6  -4.6 -12.6 -14.  -10.6 -14.8  -5.8 -10.8\n",
      " -10.   -9.2  -7.8 -16.2  -8.8 -10.8  -6.2  -8.2  -8.8 -10.   -7.2 -12.4\n",
      " -10.6 -10.4  -9.6  -4.   -7.  -16.  -13.   -6.2 -10.4 -14.2  -7.4 -13.4\n",
      "  -3.  -15.6 -17.6  -4.4 -10.  -10.2  -7.   -3.4 -15.  -14.4 -11.6  -7.2\n",
      "  -7.8 -12.2  -7.6  -6.2 -11.6  -7.2  -9.2 -14.   -4.6 -11.8  -8.8 -13.8\n",
      "  -2.8 -24.6 -14.8  -9.6  -9.8  -4.6  -8.6  -6.2 -12.6  -6.2 -15.6  -3.6\n",
      "  -7.6  -8.   -3.6 -14.8  -8.2 -13.8 -11.   -7.   -7.4  -3.2  -7.   -5.8\n",
      "  -3.2 -12.  -13.8 -14.6  -5.2 -18.8  -5.6  -9.8 -11.4 -12.2  -9.6  -7.2\n",
      "  -8.6  -9.   -6.6  -5.6  -7.8  -7.4  -5.4 -12.6  -4.4  -7.8  -7.2  -8.2\n",
      "  -4.4  -8.4  -8.4 -13.4  -9.8  -9.6 -14.4 -14.8 -11.6  -9.6 -13.2 -12.6\n",
      "  -9.   -6.   -7.   -7.8  -8.4  -8.6  -3.4  -6.6 -13.8 -11.4  -2.2 -17.2\n",
      "  -9.   -7.   -7.8  -6.4 -12.4  -8.2  -6.  -11.4 -12.8  -7.   -7.2 -15.8\n",
      "  -2.4 -13.  -14.4  -7.  -14.6 -10.4 -14.6 -12.  -14.8 -16.  -23.4 -10.6\n",
      " -12.6 -15.  -12.8  -7.6 -10.2  -8.2 -22.  -12.4  -5.4  -8.4 -12.   -7.8\n",
      "  -4.6 -16.4 -11.   -6.2 -21.2 -10.8  -8.   -6.   -4.   -9.2  -5.4 -10.8\n",
      " -12.2  -8.8  -8.4  -8.   -9.   -6.  -10.2  -7.4 -11.   -7.2  -6.2 -16.2\n",
      "  -8.6  -7.2 -18.6 -18.4  -5.2  -8.2  -6.   -5.4  -9.   -4.6  -6.8 -15.6\n",
      " -10.   -6.2 -13.8  -9.4  -7.4 -13.2 -17.6  -8.   -7.4 -19.6 -15.  -12.4\n",
      "  -8.2 -10.4  -9.2  -9.   -8.8  -8.2  -9.8  -9.4  -8.4  -2.2  -8.4 -20.6\n",
      "  -9.4 -13.   -8.8 -16.   -8.   -7.2  -8.6 -10.6  -8.6  -9.4  -7.2  -9.6\n",
      " -14.8  -5.6 -14.   -3.4 -10.6  -8.  -19.2  -9.   -6.   -9.2  -8.   -7.8\n",
      "  -5.6  -9.8 -11.  -18.2  -5.4 -12.2 -11.2  -2.2  -9.4  -6.   -7.2  -5.4\n",
      "  -5.4  -5.4  -3.6 -15.6 -12.8  -9.8 -17.6 -10.8 -10.6  -9.8  -3.6 -15.8\n",
      "  -9.8  -9.   -6.6  -6.   -6.4  -6.8 -11.6 -11.8  -8.4 -14.4 -14.4  -9.2\n",
      " -12.  -14.4  -9.  -17.8 -14.8 -12.6 -17.   -6.6 -12.6 -10.2 -13.8  -7.6\n",
      "  -4.2  -9.  -10.6  -5.2 -12.4  -6.8  -5.6 -10.2 -14.8  -5.  -12.8  -1.6\n",
      " -12.4  -9.8  -5.   -2.4 -12.8  -8.8  -8.2  -9.   -9.4  -6.   -8.   -9.6\n",
      " -10.   -4.8  -6.6 -10.6 -13.8 -11.  -11.4  -6.  -23.2 -12.4  -2.2  -9.8\n",
      "  -9.4 -10.2 -15.2  -6.6  -6.2 -15.  -15.8  -8.2 -11.  -11.   -6.4  -5.6\n",
      " -12.4  -9.8 -15.  -18.6  -5.  -12.2  -6.8  -8.2  -9.6 -14.8 -22.4  -5.4\n",
      "  -8.6  -8.2 -13.  -10.4  -6.2  -9.4 -13.  -10.   -6.8  -9.2  -7.8  -8.2\n",
      "  -9.8 -17.4  -8.   -7.  -14.2 -10.8  -8.2  -8.4  -7.2 -11.4 -11.2 -10.4\n",
      " -11.   -8.2 -13.6 -16.2  -3.2  -4.2  -7.8  -6.8  -9.8  -7.   -6.8  -8.4\n",
      "  -6.   -7.4  -4.  -10.4  -7.  -11.  -16.   -6.2 -12.2 -15.   -7.6  -8.8\n",
      " -11.4 -10.4 -15.4  -8.2 -10.8  -7.6  -9.6  -7.2]\n",
      "Q-Values:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-3.         -3.         -3.         -1.        ]\n",
      " [-4.         -4.         -4.         -2.        ]\n",
      " [-5.         -3.         -5.         -3.        ]\n",
      " [-1.         -3.         -3.         -3.        ]\n",
      " [-2.         -4.         -4.         -2.        ]\n",
      " [-3.         -3.         -3.         -3.        ]\n",
      " [-4.         -2.         -4.         -4.        ]\n",
      " [-2.         -4.         -4.         -4.        ]\n",
      " [-3.         -3.         -3.         -3.        ]\n",
      " [-4.         -2.         -2.         -4.        ]\n",
      " [-3.         -1.         -3.         -2.99999998]\n",
      " [-3.         -5.         -3.         -4.99999996]\n",
      " [-4.         -4.         -2.         -3.99999991]\n",
      " [-3.         -3.         -1.         -3.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Q-LEARNING\")\n",
    "print()\n",
    "\n",
    "discount_factor = 1 #undiscounted\n",
    "\n",
    "start = time.time()\n",
    "q_values, opt_pol, prev_opt_pol = QLearning(num_states, num_actions, actions, trans_prob, rewrds, discount_factor)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken:  1.3374\n",
      "\n",
      "Optimal Policy: (-1 represents No Policy for Terminal States)\n",
      "Policy-0: UP, 1:DOWN, 2:RIGHT, 3:LEFT\n",
      "State:  0      Policy:  -1\n",
      "State:  1      Policy:  3\n",
      "State:  2      Policy:  3\n",
      "State:  3      Policy:  1\n",
      "State:  4      Policy:  0\n",
      "State:  5      Policy:  0\n",
      "State:  6      Policy:  0\n",
      "State:  7      Policy:  1\n",
      "State:  8      Policy:  0\n",
      "State:  9      Policy:  2\n",
      "State:  10      Policy:  1\n",
      "State:  11      Policy:  1\n",
      "State:  12      Policy:  2\n",
      "State:  13      Policy:  2\n",
      "State:  14      Policy:  2\n",
      "State:  15      Policy:  -1\n",
      "\n",
      "Actions\n",
      "END  |  <  |  <  |  v\n",
      "-----------------------\n",
      "  ^  |  ^  |  ^  |  v\n",
      "-----------------------\n",
      "  ^  |  >  |  v  |  v\n",
      "-----------------------\n",
      "  >  |  >  |  >  |  END\n",
      "-----------------------\n",
      "\n",
      "Policy from previous iteration (before last iteration):\n",
      "State:  0      Policy:  -1\n",
      "State:  1      Policy:  3\n",
      "State:  2      Policy:  3\n",
      "State:  3      Policy:  1\n",
      "State:  4      Policy:  0\n",
      "State:  5      Policy:  0\n",
      "State:  6      Policy:  1\n",
      "State:  7      Policy:  1\n",
      "State:  8      Policy:  0\n",
      "State:  9      Policy:  3\n",
      "State:  10      Policy:  1\n",
      "State:  11      Policy:  1\n",
      "State:  12      Policy:  0\n",
      "State:  13      Policy:  2\n",
      "State:  14      Policy:  2\n",
      "State:  15      Policy:  -1\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken: \", round(end - start, 4))\n",
    "print()\n",
    "\n",
    "print(\"Optimal Policy: (-1 represents No Policy for Terminal States)\")\n",
    "print(\"Policy-0: UP, 1:DOWN, 2:RIGHT, 3:LEFT\")\n",
    "for key,val in opt_pol.items():\n",
    "    print(\"State: \", key, \"     Policy: \", val)\n",
    "\n",
    "print()\n",
    "\n",
    "actions_taken = [\"\" for x in range(len(opt_pol))]\n",
    "\n",
    "for i in range(0, len(opt_pol)):\n",
    "    if(opt_pol[i] == -1): actions_taken[i] = \"END\"\n",
    "    if(opt_pol[i] == 0): actions_taken[i] = \"^\"\n",
    "    if(opt_pol[i] == 1): actions_taken[i] = \"v\"\n",
    "    if(opt_pol[i] == 2): actions_taken[i] = \">\"\n",
    "    if(opt_pol[i] == 3): actions_taken[i] = \"<\"\n",
    "        \n",
    "print(\"Actions\")\n",
    "print(actions_taken[0], \" | \", actions_taken[1], \" | \", actions_taken[2], \" | \", actions_taken[3])\n",
    "print(\"-----------------------\")\n",
    "print(\" \", actions_taken[4], \" | \", actions_taken[5], \" | \", actions_taken[6], \" | \", actions_taken[7])\n",
    "print(\"-----------------------\")\n",
    "print(\" \", actions_taken[8], \" | \", actions_taken[9], \" | \", actions_taken[10], \" | \", actions_taken[11])\n",
    "print(\"-----------------------\")\n",
    "print(\" \", actions_taken[12], \" | \", actions_taken[13], \" | \", actions_taken[14], \" | \", actions_taken[15])\n",
    "print(\"-----------------------\")\n",
    "print()\n",
    "\n",
    "print(\"Policy from previous iteration (before last iteration):\")\n",
    "for key,val in prev_opt_pol.items():\n",
    "    print(\"State: \", key, \"     Policy: \", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
